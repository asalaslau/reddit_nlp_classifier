{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870d1768-477a-4ff1-b95d-dda9002aed02",
   "metadata": {},
   "source": [
    "# **1. Data Recolection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c7e4c9-230b-4196-afd5-9ee5709e496f",
   "metadata": {},
   "source": [
    "Actualmente en ***Too Good To Go*** no disponemos de un servicio de **food delivering** para poder llegar a un publico mas amplio promocionando **habitos de consumo** que **mejoren nuestra sociedad**. El ano pasado tuvimos una venta total de 121 millones de meals, sabemos que compararnos con empresas como Doordash que suponen mas de 2161 millones de delivery orders puede ser ambicio, pero aun si es posible. \n",
    "\n",
    "Por esta razon en este proyecto trabajaremos en el analisis de los usuarios de food ordering/delivering de las empresas de ***Doordash*** y ***UberEats*** en la red social ***Reddit***. Ambas suponen **mas del 80% del mercado**. Con esta informacion podemos profundizar en el perfil de sus clientes y repartidores. Como en cualquier otra mercado, el cliente siempre tiene la razon, pero este en particular depende de los repartidores que son la cara de tu empresa.\n",
    "\n",
    "Problemas que podemos encontrar.\n",
    "* Unicamente enfocarnos en Reddit puede influer en no tener la foto completa de que esta ocurriendo.\n",
    "* Existen otras companias de food delivering que no estamos teniendo en cuenta en nuestro estudio.\n",
    "* Posibilidad de no existencia de diferencias entre empresas para Reddit.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5742f351-bf64-4886-bfca-dae7d0feb78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re #Regular expression operation - \n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"config.json\", \"r\") as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "reddit = praw.Reddit( \n",
    "                    client_id = config[\"client_id\"],\n",
    "                    client_secret = config[\"client_secret\"],\n",
    "                    user_agent = config[\"user_agent\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f4035b-7cfe-43b9-a402-a626c132ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# What subreddit are important to us\n",
    "subreddit = reddit.subreddit('doordash')\n",
    "#subreddit = reddit.subreddit('UberEATS')\n",
    "\n",
    "# List to save our post\n",
    "data = []\n",
    "\n",
    "# Save the 1000 newsest posts from our reddit\n",
    "for post in subreddit.new(limit=1000):  # Obtiene los 1000 posts más recientes\n",
    "    data.append([\n",
    "        post.created_utc,          # Fecha de creación en formato UTC\n",
    "        post.title,               # Título del post\n",
    "        post.selftext,            # Texto del post (si es un post de texto)\n",
    "        post.subreddit.display_name,  # Nombre del subreddit\n",
    "        post.num_comments,        # Número de comentarios\n",
    "        post.score,               # Puntos de la publicación (score)\n",
    "        post.upvote_ratio,        # Ratio de upvotes\n",
    "        post.url,                 # URL del post\n",
    "        post.is_self,             # Si el post es de tipo \"self post\"\n",
    "        post.link_flair_text,     # Flair del post (puede ser None si no tiene flair)\n",
    "        post.author.name if post.author else None,  # Autor del post\n",
    "        post.num_reports,         # Número de reportes\n",
    "        post.distinguished,       # Si el post es distinguido (sticky, gold, etc.)\n",
    "        post.over_18,             # Si es un post NSFW\n",
    "    ])\n",
    "\n",
    "\n",
    "# Transform to a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\n",
    "    'created_utc', 'title', 'self_text', 'subreddit', 'num_comments', \n",
    "    'score', 'upvote_ratio', 'url', 'is_self', 'link_flair_text', \n",
    "    'author', 'num_reports', 'distinguished', 'over_18'\n",
    "])\n",
    "\n",
    "\n",
    "# Set date for our CSV creation\n",
    "current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# Save the CSV with date info\n",
    "file_name = f'subreddit_new_posts_{current_time}.csv'\n",
    "df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4deae6-2ff8-4146-9844-eed9afa7b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the files create for our analysis\n",
    "files = os.listdir('Datasets')\n",
    "csv_files = [f for f in files if f.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5edf8fed-80a5-4cac-9573-8271f70690f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "\n",
    "# Import files from CSV with pandas on the same list of dataframes \n",
    "# Create new vars to catalogue every record from every archive\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join('Datasets', file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "    \n",
    "    if len(file.split(\"_\"))==5:\n",
    "        df['file_date']=file.split(\"_\")[3]\n",
    "        df['file_hour']=file.split(\"_\")[4]        \n",
    "        df['file_type']=file.split(\"_\")[1]\n",
    "    else:\n",
    "        df['file_date']=file.split(\"_\")[2]\n",
    "        df['file_hour']=file.split(\"_\")[3]        \n",
    "        df['file_type']='new'\n",
    "    dataframes.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bdafc1d-099f-4293-aae7-063219118f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Verifica el contenido del primer dataframe\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "#combined_df.columns\n",
    "#combined_df[(combined_df['created_utc.1'] != combined_df['created_utc']) & combined_df['created_utc.1'].notnull()]\n",
    "combined_df=combined_df.drop(columns='created_utc.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fbb1407-92fc-49b8-8051-8ba39629872a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>title</th>\n",
       "      <th>self_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>url</th>\n",
       "      <th>file_date</th>\n",
       "      <th>file_hour</th>\n",
       "      <th>file_type</th>\n",
       "      <th>is_self</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>over_18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.731987e+09</td>\n",
       "      <td>First time driving tonight ... Is this normal???</td>\n",
       "      <td>I drove for 2.5 hours doing Uber Eats for the ...</td>\n",
       "      <td>UberEATS</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/UberEATS/comments/1gu...</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>19-38-10.csv</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.731983e+09</td>\n",
       "      <td>Referral rewards no longer working on pickup o...</td>\n",
       "      <td>Referral rewards used to work on pickup buti c...</td>\n",
       "      <td>UberEATS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>https://www.reddit.com/r/UberEATS/comments/1gu...</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>19-38-10.csv</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.731980e+09</td>\n",
       "      <td>Does Ubereats even have support</td>\n",
       "      <td>I called the support number, tried livechat, s...</td>\n",
       "      <td>UberEATS</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>https://www.reddit.com/r/UberEATS/comments/1gu...</td>\n",
       "      <td>2024-11-18</td>\n",
       "      <td>19-38-10.csv</td>\n",
       "      <td>new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc                                              title  \\\n",
       "0  1.731987e+09   First time driving tonight ... Is this normal???   \n",
       "1  1.731983e+09  Referral rewards no longer working on pickup o...   \n",
       "2  1.731980e+09                    Does Ubereats even have support   \n",
       "\n",
       "                                           self_text subreddit  num_comments  \\\n",
       "0  I drove for 2.5 hours doing Uber Eats for the ...  UberEATS             2   \n",
       "1  Referral rewards used to work on pickup buti c...  UberEATS             1   \n",
       "2  I called the support number, tried livechat, s...  UberEATS             2   \n",
       "\n",
       "   score  upvote_ratio                                                url  \\\n",
       "0      1           1.0  https://www.reddit.com/r/UberEATS/comments/1gu...   \n",
       "1      1           1.0  https://www.reddit.com/r/UberEATS/comments/1gu...   \n",
       "2      0           0.5  https://www.reddit.com/r/UberEATS/comments/1gu...   \n",
       "\n",
       "    file_date     file_hour file_type is_self link_flair_text author  \\\n",
       "0  2024-11-18  19-38-10.csv       new     NaN             NaN    NaN   \n",
       "1  2024-11-18  19-38-10.csv       new     NaN             NaN    NaN   \n",
       "2  2024-11-18  19-38-10.csv       new     NaN             NaN    NaN   \n",
       "\n",
       "   num_reports distinguished over_18  \n",
       "0          NaN           NaN     NaN  \n",
       "1          NaN           NaN     NaN  \n",
       "2          NaN           NaN     NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04542098-6fa0-4731-a281-008dd325d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df=combined_df.sort_values(by=['file_date','file_hour'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "538bcf5d-1929-4602-b89e-9eac49907df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rows = combined_df.drop_duplicates(subset=['created_utc', 'title', 'self_text', 'subreddit','author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17adac2a-eb8a-4bcf-9c4b-dbdf3716f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rows = unique_rows.drop_duplicates(subset=['title','self_text','author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "511a7f05-d75c-49fc-9d1c-dc55282cd101",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rows = combined_df.drop_duplicates(subset=['created_utc', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a70662e-d4e0-48fa-925c-d351d13ec8e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_utc           0\n",
       "title                 0\n",
       "self_text           367\n",
       "subreddit             0\n",
       "num_comments          0\n",
       "score                 0\n",
       "upvote_ratio          0\n",
       "url                   0\n",
       "file_date             0\n",
       "file_hour             0\n",
       "file_type             0\n",
       "is_self             328\n",
       "link_flair_text    2371\n",
       "author              348\n",
       "num_reports        2727\n",
       "distinguished      2724\n",
       "over_18             328\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2687\n",
    "#unique_rows[unique_rows.duplicated(subset=['title'],keep=False)].sort_values(by=['title'],ascending=False)\n",
    "unique_rows.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "043cdad5-7004-478c-850e-ba9a40861cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rows=unique_rows.drop(columns=['num_reports','distinguished','over_18'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74b7af0c-fd87-41ff-932e-8d4d7866c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rows['info_url']= unique_rows['url'].apply(lambda x: x.split('/')[-1] \n",
    "                             if not x.endswith('/') \n",
    "                             else None\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a9f4b3d-2b80-4222-b1cd-c5a77967095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#info_url has a media attached\n",
    "#the selftext has a media description(photo...)\n",
    "unique_rows['media'] = unique_rows['info_url'].apply(lambda x: 'y' \n",
    "                              if x is not None \n",
    "                              else 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4309bda5-82eb-4e13-9b87-b31e12dea3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_utc           0\n",
       "title                 0\n",
       "self_text           367\n",
       "subreddit             0\n",
       "num_comments          0\n",
       "score                 0\n",
       "upvote_ratio          0\n",
       "url                   0\n",
       "file_date             0\n",
       "file_hour             0\n",
       "file_type             0\n",
       "is_self             328\n",
       "link_flair_text    2371\n",
       "author              348\n",
       "info_url           1709\n",
       "media                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_rows.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42153085-2b24-4d53-8e9e-6d6ef269c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rows.to_csv('post_reddit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
