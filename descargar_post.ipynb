{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62609604-edd2-49d2-954d-3405a95bc472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id='DJRjqLkCiZFZg9eFnJGqig',\n",
    "    client_secret='qSIzDKVXtWIHhyVLpIv7nGmK1813rA',\n",
    "    user_agent='as-lecturas-ga'   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ba20fdb-1377-47bf-b7a9-4666c2d0fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is JUST an example of how you can use PRAW\n",
    "\n",
    "# Choose your subreddit\n",
    "subreddit = reddit.subreddit('doordash')\n",
    "\n",
    "# Adjust the limit as needed -- Note that this will grab the 25 most recent posts\n",
    "posts = subreddit.new(limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04afb0ab-e103-4d40-b217-f3e3dc89edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "subreddit = reddit.subreddit('doordash')\n",
    "\n",
    "# Ejemplo: obteniendo publicaciones de un subreddit\n",
    "for post in subreddit.new(limit=1000):  # 'subreddit' debe ser tu objeto subreddit, por ejemplo, 'reddit.subreddit(\"python\")'\n",
    "    data.append([post.created_utc, post.title, post.selftext, post.subreddit,post.num_comments, post.score, post.upvote_ratio, post.url])\n",
    "# Crear el DataFrame correctamente\n",
    "data = pd.DataFrame(data, columns = ['created_utc', \n",
    "                                     'title', \n",
    "                                     'self_text', \n",
    "                                     'subreddit', \n",
    "                                     'num_comments', \n",
    "                                     'score', \n",
    "                                     'upvote_ratio', \n",
    "                                     'url'])\n",
    "data.head()\n",
    "\n",
    "# Obtener la fecha actual en formato YYYY-MM-DD_HH-MM-SS\n",
    "current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# Guardar los datos en un archivo CSV con la fecha actual en el nombre\n",
    "file_name = f'subreddit_new_posts_{current_time}.csv'\n",
    "data.to_csv(file_name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282bf7d-7960-489d-984d-f72171b8cfdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b008c-afd7-4b13-bd19-dda4511f4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('UberEATS')\n",
    "data =[]\n",
    "for post in subreddit.new(limit=1000):  # 'subreddit' debe ser tu objeto subreddit, por ejemplo, 'reddit.subreddit(\"python\")'\n",
    "    data.append([post.created_utc, post.title, post.selftext, post.subreddit, post.num_comments, post.score, post.upvote_ratio, post.url])\n",
    "# Crear el DataFrame correctamente\n",
    "data = pd.DataFrame(data, columns = ['created_utc', 'title', 'self_text', 'subreddit', 'num_comments', 'score', 'upvote_ratio', 'url'])\n",
    "data.head()\n",
    "\n",
    "# Obtener la fecha actual en formato YYYY-MM-DD_HH-MM-SS\n",
    "current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# Guardar los datos en un archivo CSV con la fecha actual en el nombre\n",
    "file_name = f'subreddit_new_posts_{current_time}.csv'\n",
    "data.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c81fc2b-e4ed-4c06-9f4e-5cf04f3ed601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    created_utc                                              title  \\\n",
      "0  1.732392e+09                Sudden drop in Satisfaction Ratings   \n",
      "1  1.732389e+09  Puzzling and Bizarre - “The straws are over th...   \n",
      "2  1.732384e+09      Driver Had To Drop Someone Off At The Airport   \n",
      "3  1.732382e+09                        Order cancelled, no refund    \n",
      "4  1.732381e+09                                    UberEats Walker   \n",
      "\n",
      "                                           self_text subreddit  num_comments  \\\n",
      "0  Uber Stuff\\n\\n\\n\\nMy satisfaction numbers went...  UberEATS             2   \n",
      "1  Over time, I’ve become more detail oriented wi...  UberEATS             7   \n",
      "2  Got home late last night and had a few drinks ...  UberEATS             3   \n",
      "3  Hi, could anyone please advise me on what to d...  UberEATS             1   \n",
      "4  I decided to sign up for UberEats and choose t...  UberEATS             1   \n",
      "\n",
      "   score  upvote_ratio                                                url  \\\n",
      "0      1           1.0  https://www.reddit.com/r/UberEATS/comments/1gy...   \n",
      "1      2           1.0  https://www.reddit.com/r/UberEATS/comments/1gy...   \n",
      "2      6           1.0  https://www.reddit.com/r/UberEATS/comments/1gy...   \n",
      "3      1           1.0  https://www.reddit.com/r/UberEATS/comments/1gy...   \n",
      "4      1           1.0  https://www.reddit.com/r/UberEATS/comments/1gy...   \n",
      "\n",
      "   is_self link_flair_text         author   created_utc num_reports  \\\n",
      "0     True            None   jcsoside7804  1.732392e+09        None   \n",
      "1     True            None  spacecity2018  1.732389e+09        None   \n",
      "2     True            None       jtlovato  1.732384e+09        None   \n",
      "3     True            None      freyaw100  1.732382e+09        None   \n",
      "4     True             USA        Neon726  1.732381e+09        None   \n",
      "\n",
      "  distinguished  over_18  \n",
      "0          None    False  \n",
      "1          None    False  \n",
      "2          None    False  \n",
      "3          None    False  \n",
      "4          None    False  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Obtenemos el subreddit que nos interesa\n",
    "subreddit = reddit.subreddit('UberEATS')\n",
    "\n",
    "# Lista para almacenar los datos de las publicaciones\n",
    "data = []\n",
    "\n",
    "# Recopilar los primeros 1000 posts nuevos en el subreddit\n",
    "for post in subreddit.new(limit=1000):  # Obtiene los 1000 posts más recientes\n",
    "    data.append([\n",
    "        post.created_utc,          # Fecha de creación en formato UTC\n",
    "        post.title,               # Título del post\n",
    "        post.selftext,            # Texto del post (si es un post de texto)\n",
    "        post.subreddit.display_name,  # Nombre del subreddit\n",
    "        post.num_comments,        # Número de comentarios\n",
    "        post.score,               # Puntos de la publicación (score)\n",
    "        post.upvote_ratio,        # Ratio de upvotes\n",
    "        post.url,                 # URL del post\n",
    "        post.is_self,             # Si el post es de tipo \"self post\"\n",
    "        post.link_flair_text,     # Flair del post (puede ser None si no tiene flair)\n",
    "        post.author.name if post.author else None,  # Autor del post\n",
    "        post.created_utc,         # Tiempo de creación en formato UTC\n",
    "        post.num_reports,         # Número de reportes\n",
    "        post.distinguished,       # Si el post es distinguido (sticky, gold, etc.)\n",
    "        post.over_18,             # Si es un post NSFW\n",
    "    ])\n",
    "\n",
    "\n",
    "# Crear el DataFrame correctamente\n",
    "df = pd.DataFrame(data, columns=[\n",
    "    'created_utc', 'title', 'self_text', 'subreddit', 'num_comments', \n",
    "    'score', 'upvote_ratio', 'url', 'is_self', 'link_flair_text', \n",
    "    'author', 'created_utc', 'num_reports', 'distinguished', 'over_18'\n",
    "])\n",
    "\n",
    "# Ver el DataFrame para asegurarse de que la información está correcta\n",
    "print(df.head())\n",
    "\n",
    "# Obtener la fecha actual en formato YYYY-MM-DD_HH-MM-SS\n",
    "current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# Guardar los datos en un archivo CSV con la fecha actual en el nombre\n",
    "file_name = f'subreddit_new_posts_{current_time}.csv'\n",
    "df.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "689ad30b-6085-4992-a01a-01482f4c8080",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'subreddit_new_posts_{current_time}.csv'\n",
    "df.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4416c42d-8536-460d-b43d-1bf5841785a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la solicitud a la API de Pushshift: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import praw\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configura PRAW\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id='DJRjqLkCiZFZg9eFnJGqig',\n",
    "    client_secret='qSIzDKVXtWIHhyVLpIv7nGmK1813rA',\n",
    "    user_agent='as-lecturas-ga'   \n",
    ")\n",
    "\n",
    "# Define el rango de fechas\n",
    "end_date = datetime(2023, 11, 20)  # Fecha final (20 de noviembre)\n",
    "start_date = end_date - timedelta(days=5)  # Fecha inicial (15 de noviembre)\n",
    "\n",
    "# Convertir a timestamp Unix\n",
    "end_timestamp = int(end_date.timestamp())\n",
    "start_timestamp = int(start_date.timestamp())\n",
    "\n",
    "# Configurar parámetros para la consulta a Pushshift\n",
    "subreddit_name = \"doordash\"  # Reemplaza con el nombre de tu subreddit\n",
    "url = (\n",
    "    f\"https://api.pushshift.io/reddit/search/submission/\"\n",
    "    f\"?subreddit={subreddit_name}\"\n",
    "    f\"&after={start_timestamp}&before={end_timestamp}\"\n",
    "    f\"&size=200\"  # Número máximo de resultados por solicitud\n",
    ")\n",
    "\n",
    "# Realiza la solicitud a Pushshift\n",
    "# Realiza la solicitud a Pushshift\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verifica si la respuesta contiene datos\n",
    "if response.status_code == 200:  # Solicitud exitosa\n",
    "    try:\n",
    "        json_data = response.json()  # Convertir respuesta a JSON\n",
    "        if \"data\" in json_data and json_data[\"data\"]:  # Verifica si hay datos\n",
    "            data = json_data[\"data\"]\n",
    "\n",
    "            # Almacena los IDs de las publicaciones\n",
    "            post_ids = [post[\"id\"] for post in data]\n",
    "\n",
    "            # Continúa con el procesamiento\n",
    "            print(f\"Se encontraron {len(post_ids)} publicaciones en el rango de fechas.\")\n",
    "        else:\n",
    "            print(\"La API no devolvió publicaciones en el rango solicitado.\")\n",
    "            data = []\n",
    "    except ValueError as e:\n",
    "        print(f\"Error al procesar JSON: {e}\")\n",
    "else:\n",
    "    print(f\"Error en la solicitud a la API de Pushshift: {response.status_code}\")\n",
    "    data = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a29e6-59e6-4d8e-9940-04911330a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enriquecer datos con PRAW\n",
    "posts_with_details = []\n",
    "for post_id in post_ids:\n",
    "    try:\n",
    "        praw_post = reddit.submission(id=post_id)\n",
    "        posts_with_details.append({\n",
    "            \"created_utc\": datetime.utcfromtimestamp(praw_post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"title\": praw_post.title,\n",
    "            \"selftext\": praw_post.selftext,\n",
    "            \"subreddit\": praw_post.subreddit.display_name,\n",
    "            \"num_comments\": praw_post.num_comments,\n",
    "            \"score\": praw_post.score,\n",
    "            \"upvote_ratio\": praw_post.upvote_ratio,\n",
    "            \"url\": praw_post.url,\n",
    "            \"is_self\": praw_post.is_self,\n",
    "            \"link_flair_text\": praw_post.link_flair_text,\n",
    "            \"author\": praw_post.author.name if praw_post.author else None,\n",
    "            \"over_18\": praw_post.over_18,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando post {post_id}: {e}\")\n",
    "\n",
    "# Muestra algunos resultados\n",
    "print(f\"Se encontraron {len(posts_with_details)} publicaciones en el rango de fechas.\")\n",
    "for post in posts_with_details[:10]:  # Los primeros 10 resultados\n",
    "    print(post[\"title\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
